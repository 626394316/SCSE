import json
import re
import os
import requests
import arxiv
import random
import whisper
import cv2
import numpy as np
import time
import uuid
import edge_tts
import yaml

from langchain_community.vectorstores import FAISS
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_core.utils.function_calling import convert_to_openai_tool
from langchain_core.output_parsers import JsonOutputParser
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.graphs import Neo4jGraph

from BCEmbedding import RerankerModel
from datetime import datetime
from operator import itemgetter
from bs4 import BeautifulSoup

from fastapi.middleware.cors import CORSMiddleware
from fastapi import FastAPI, File, UploadFile
from fastapi.responses import FileResponse
from pydantic import BaseModel
from fastapi.responses import StreamingResponse

import yaml
from graph_search import get_knowledge_graph

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins="*",  # *ï¼šä»£è¡¨æ‰€æœ‰å®¢æˆ·ç«¯
    allow_credentials=True,
    allow_methods=["GET,POST"],
    allow_headers=["*"],
)

with open("config.yml", 'r', encoding='utf-8') as f:
    configs = yaml.load(f.read(), Loader=yaml.FullLoader)

openai_api_base = "http://localhost:8000/v1"
openai_api_key = "none"

llm = ChatOpenAI(
    model_name=configs['llm_name'],
    openai_api_base=configs['llm_api_path'],
    openai_api_key="none",
    streaming=True,
    model_kwargs={
        "stop": ["Observation:", "Observation:\n"]
    },
    temperature=0
)

emo_llm = ChatOpenAI(
    model_name=configs['emo_llm_name'],
    openai_api_base=configs['emo_llm_api_path'],
    openai_api_key="none",
    streaming=True,
)

graph = Neo4jGraph(
    url=configs['neo4j_url'],
    username=configs['neo4j_username'],
    password=configs['neo4j_password'],
    database=configs['neo4j_database']
)

source = []

class Chatbot:
    def __init__(self):
        self.embeddings = HuggingFaceEmbeddings(model_name=configs['embedding_model_path'])
        self.vectorstore = FAISS.load_local("./vector_db/common_data", self.embeddings,
                                            allow_dangerous_deserialization=True)

        self.pr_vectorstore = FAISS.load_local("./vector_db/pr_data", self.embeddings,
                                               allow_dangerous_deserialization=True)
        self.RerankerModel = RerankerModel(model_name_or_path=configs['rerank_model_path'])

    def get_response(self, question):
        global source
        source = []
        is_stream = ""
        img_text_prompt = f"""
        ä½ æ‰®æ¼”æ–‡æœ¬åˆ†ç±»çš„å·¥å…·åŠ©æ‰‹,ç±»åˆ«æœ‰3ç§;
        å¦‚æœæ–‡æœ¬æ˜¯å…³äºæ£€ç´¢å›¾ç‰‡æˆ–è€…ç”Ÿæˆå›¾ç‰‡çš„,ä½ è¿”å›1;
        å¦‚æœæ–‡æœ¬æ˜¯å…³äºç”Ÿæˆä»£ç çš„,ä½ è¿”å›2;
        å¦‚æœæ–‡æœ¬æ˜¯å…¶ä»–ç›¸å…³çš„ï¼Œä½ è¿”å›3;

        ä»¥ä¸‹æ˜¯ä¸€äº›ä¾‹å­ï¼š
        ä¾‹å­1ï¼š'å¸®æˆ‘ç”Ÿæˆä¸€å¼ åä¾¨å¤§å­¦çš„ç…§ç‰‡',æ–‡æœ¬åˆ†ç±»ç»“æœæ˜¯1
        ä¾‹å­2:'å¸®æˆ‘ç”Ÿæˆä¸€å¼ å°ç‹—çš„å›¾ç‰‡',æ–‡æœ¬åˆ†ç±»ç»“æœæ˜¯1
        ä¾‹å­3ï¼š'å¸®æˆ‘ç”Ÿæˆä¸€æ®µå¿«é€Ÿæ’åºçš„ä»£ç ',æ–‡æœ¬åˆ†ç±»ç»“æœæ˜¯2
        ä¾‹å­4ï¼š'å¸®æˆ‘ç”Ÿæˆä¸€æ®µå“ˆå¸Œæ’åºçš„ä»£ç ',æ–‡æœ¬åˆ†ç±»ç»“æœæ˜¯2
        ä¾‹å­3ï¼š'å­¦æ ¡å›¾ä¹¦é¦†æ€ä¹ˆä¸Šç½‘',æ–‡æœ¬åˆ†ç±»ç»“æœæ˜¯3
        ä¾‹å­4ï¼š'ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±',æ–‡æœ¬åˆ†ç±»ç»“æœæ˜¯3
        è¯·å‚è€ƒä¸Šé¢ä¾‹å­ï¼Œç›´æ¥ç»™å‡ºä¸€ç§åˆ†ç±»ç»“æœï¼Œä¸è¦è§£é‡Šï¼Œä¸è¦å¤šä½™çš„å†…å®¹ï¼Œä¸è¦å¤šä½™çš„ç¬¦å·ï¼Œä¸è¦å¤šä½™çš„ç©ºæ ¼ï¼Œä¸è¦å¤šä½™çš„ç©ºè¡Œï¼Œä¸è¦å¤šä½™çš„æ¢è¡Œï¼Œä¸è¦å¤šä½™çš„æ ‡ç‚¹ç¬¦å·ï¼Œä¸è¦å¤šä½™çš„æ‹¬å·ã€‚
        è¯·ä½ å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œæ–‡æœ¬åˆ†ç±»ï¼š
        {question}

        """
        img_or_text = llm.predict(img_text_prompt)
        print("img_or_text", img_or_text)
        if img_or_text == '1':
            source = []
            p_r = build_planning_prompt(question)
            for i in range(0, 8):
                res = get_res(p_r)
                print("res", res)
                if res.find("Action Input:") != -1:
                    args = get_args(res)
                    t_run = tool_chain(args)
                    p_r = p_r + res + "\nObservation: " + str(t_run.invoke(args)) + "\n"
                else:
                    p_r = p_r + res
                    p_r = p_r.split("Final Answer:")[-1]
                    break
            is_stream, llm_res = get_same_response(p_r)
            return is_stream, llm_res
        elif img_or_text == '2':
            source = []
            llm_res = llm.stream(question)
            return 1, llm_res
        elif img_or_text == '3':
            docs = self.vectorstore.similarity_search(query=question, k=1)
            passages = []
            for doc in docs:
                passages.append(doc.page_content)
            rerank_results = self.RerankerModel.rerank(query=question, passages=passages)
            print(rerank_results["rerank_scores"][0])
            if rerank_results["rerank_scores"][0] > 0.5:
                for doc in docs:
                    source.append({
                        'link': doc.metadata['source'],
                        'title': doc.metadata['source'].split('static/')[1].split('/')[1]
                    })
                print("source", source)
                prompt = f"""
                  ä½ æ˜¯åä¾¨å¤§å­¦å¼€å‘çš„AIåŠ©æ‰‹,è¯·ä½ æ ¹æ®ä¸‹åˆ—çŸ¥è¯†åº“çš„å†…å®¹æ¥å›ç­”é—®é¢˜,
                  å¦‚æœæ— æ³•ä»ä¸­å¾—åˆ°ç­”æ¡ˆ,è¯·è¯´"æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶ä¸çŸ¥é“å¦‚ä½•è§£ç­”è¯¥é—®é¢˜"å³å¯,ä¸å…è®¸åœ¨ç­”æ¡ˆä¸­æ·»åŠ ç¼–é€ æˆåˆ†å’Œå¤šä½™å†…å®¹

                  ä»¥ä¸‹æ˜¯çŸ¥è¯†åº“:
                  {rerank_results["rerank_passages"][0]}
                  ä»¥ä¸Šæ˜¯çŸ¥è¯†åº“;

                  ç”¨æˆ·é—®é¢˜:
                  {question}
                  """
                llm_res = llm.stream(prompt)
            else:
                source = []
                p_r = build_planning_prompt(question)
                for i in range(0, 8):
                    res = get_res(p_r)
                    print("res", res)
                    if res.find("Action Input:") != -1:
                        args = get_args(res)
                        t_run = tool_chain(args)
                        p_r = p_r + res + "\nObservation: " + str(t_run.invoke(args)) + "\n"
                    else:
                        p_r = p_r + res
                        p_r = p_r.split("Final Answer:")[-1]
                        break
                is_stream, llm_res = get_same_response(p_r)

            return is_stream, llm_res

    def get_hqu_img(self, question):
        if "é£æ™¯" in question:
            image_path = "/web_demo/src/assets/llm_img/åä¾¨å¤§å­¦é£æ™¯ç…§.jpg"
        elif "å¤œæ™¯" in question:
            image_path = "/web_demo/src/assets/llm_img/åä¾¨å¤§å­¦å¤œæ™¯.jpg"
        elif "å›¾ä¹¦é¦†" in question:
            image_path = "/web_demo/src/assets/llm_img/åä¾¨å¤§å­¦å›¾ä¹¦é¦†.jpg"
        elif "logo" in question:
            image_path = "/web_demo/src/assets/llm_img/åä¾¨å¤§å­¦logo.jpg"
        else:
            image_path = "/web_demo/src/assets/llm_img/åä¾¨å¤§å­¦other.jpg"
        return image_path

    # å¾—åˆ°æ¨¡å¼è¯†åˆ«è¯¾ç¨‹çš„å›ç­”
    def get_pr_exam_response(self, question):
        global source
        source = []
        graph_result = get_knowledge_graph(question, llm, graph)
        docs = self.pr_vectorstore.similarity_search(query=question, k=1)
        passages = []
        for doc in docs:
            passages.append(doc.page_content)
            source.append({
                'link': doc.metadata['source'],
                'title': doc.metadata['source'].split('static/')[1].split('/')[1]
            })
        print("source", source)
        print("graph_result", graph_result)
        # pr_vectorstore
        prompt = f"""
                You are a helpful question-answering agent. Your task is to analyze 
        and synthesize information from two sources: the top result from a similarity search 
        (unstructured information) and relevant data from a graph database (structured information). 
        Given the user's query: {question}, provide a meaningful and efficient answer in chinese based 
        on the insights derived from the following data:

        Unstructured information: {docs[0].page_content}. 
        Structured information: {graph_result}.

        note:Answer the question by referring only to two sources and adding nothing extra.
        note:If the two sources information contains latex formula,please return to the announcement intact.
        It is very important!
        """
        # print("prompt", prompt)
        result = llm.stream(prompt)
        print("result", result)
        return result

    def get_Emo_response(self, user_input):
        prompt = f"""
        ç°åœ¨ä½ æ˜¯ä¸€ä¸ªç ”ç©¶è¿‡æ— æ•°å…·æœ‰å¿ƒç†å¥åº·é—®é¢˜çš„ç—…äººä¸å¿ƒç†å¥åº·åŒ»ç”Ÿå¯¹è¯çš„å°Vä¸“å®¶,
        ç”¨æˆ·æœ‰ä¸€äº›å¿ƒç†é—®é¢˜, è¯·ä½ ä¸€æ­¥æ­¥è¯±å¯¼ç—…äººè¯´å‡ºè‡ªå·±çš„é—®é¢˜è¿›è€Œæä¾›è§£å†³é—®é¢˜çš„å¯è¡Œæ–¹æ¡ˆã€‚
        å¦‚æœä½ å›å¤çš„å¥½ï¼Œæˆ‘ä¼šç»™ä½ ä¸€å®šçš„èµé‡‘ã€‚
        ç”¨æˆ·é—®é¢˜ï¼š{user_input}
        """
        print("Emo", prompt)
        result = emo_llm.stream(prompt)
        return result


# è¿”å›æµå¼è¾“å‡ºçš„å›ç­”
def get_same_response(answer):
    if (".jpg" in answer):
        pattern = r'/web_demo.*?\.jpg'
        matches = re.findall(pattern, answer, re.IGNORECASE)
        if matches and matches[0] in answer:
            return 0, matches[0]
    else:
        prompt = f"""
        ä½ åªéœ€è¦åŸå°ä¸åŠ¨çš„è¿”å›ç­”æ¡ˆçš„å†…å®¹å³å¯ï¼Œä¸€ä¸ªå­—ä¹Ÿä¸è¦ä¿®æ”¹ï¼Œä¹Ÿä¸è¦æ·»åŠ ä»»ä½•å¤šä½™çš„å†…å®¹ï¼Œè¿™å¯¹æˆ‘ååˆ†é‡è¦ã€‚
        ä»¥ä¸‹æ˜¯ç­”æ¡ˆï¼š
        {answer}
        ä»¥ä¸Šæ˜¯ç­”æ¡ˆ;
        """

        result = llm.stream(prompt)
        return 1, result


# è¿”å›å¤§æ¨¡å‹çš„è‡ªæˆ‘ä»‹ç»
def self_qa():
    result_list = [
        "ä½ å¥½ğŸ˜Šï¼Œæˆ‘æ˜¯åä¾¨å¤§å­¦å¼€å‘çš„æ ¡å›­AIåŠ©æ‰‹å°Hï¼Œæˆ‘æ—¨åœ¨ä¸ºå­¦ç”Ÿå’Œè€å¸ˆæä¾›æ›´ä¾¿æ·ã€é«˜æ•ˆçš„æœåŠ¡ã€‚",
        "ä½ å¥½ğŸ˜Šï¼Œæˆ‘æ˜¯åä¾¨å¤§å­¦ç²¾å¿ƒæ‰“é€ çš„æ ¡å›­AIåŠ©æ‰‹å°Hï¼Œæˆ‘çš„ç›®æ ‡æ˜¯ä¸ºæ ¡å›­ä¸­çš„å¤§å®¶æä¾›æ›´åŠ æ™ºèƒ½ã€ä¸ªæ€§åŒ–çš„æœåŠ¡ä½“éªŒã€‚",
        "ä½ å¥½ğŸ˜Šï¼Œæˆ‘æ˜¯åä¾¨å¤§å­¦ç²¾å¿ƒå­•è‚²çš„æ ¡å›­AIåŠ©æ‰‹å°Hï¼Œæˆ‘åœ¨è¿™é‡Œï¼Œå°±æ˜¯ä¸ºäº†ç»™æ ¡å›­ç”Ÿæ´»å¸¦æ¥ä¸€æŠ¹æ™ºèƒ½çš„è‰²å½©ï¼Œè®©å­¦ä¹ ã€å·¥ä½œå˜å¾—æ›´åŠ è½»æ¾æ„‰å¿«ã€‚",
        "ä½ å¥½ğŸ˜Šï¼Œæˆ‘æ˜¯åä¾¨å¤§å­¦å€¾åŠ›æ‰“é€ çš„æ ¡å›­AIåŠ©æ‰‹å°Hï¼Œæˆ‘çš„å­˜åœ¨ï¼Œå°±æ˜¯ä¸ºäº†è®©ä½ åœ¨æ ¡å›­ä¸­çš„æ¯ä¸€å¤©éƒ½æ›´åŠ ç²¾å½©ï¼Œæ— è®ºæ˜¯å­¦æœ¯æ¢è®¨è¿˜æ˜¯æ—¥å¸¸ç”Ÿæ´»ï¼Œæˆ‘éƒ½å°†æ˜¯ä½ è´´å¿ƒçš„æ™ºèƒ½ä¼™ä¼´ã€‚"
    ]
    result = random.choices(result_list)
    return result


# æ–‡æœ¬åˆ†ç±»
def query_classify(question):
    prompt = f"""
    ä½ æ‰®æ¼”æ–‡æœ¬åˆ†ç±»çš„å·¥å…·åŠ©æ‰‹,ç±»åˆ«æœ‰2ç§;
    å¦‚æœæ–‡æœ¬æ˜¯å…³äºè¦æ±‚ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±çš„æˆ–è€…é—®å€™ä½ çš„,ä½ è¿”å›1;
    å¦‚æœæ–‡æœ¬æ˜¯å…¶ä»–ç›¸å…³çš„ï¼Œä½ è¿”å›2;
    ä»¥ä¸‹æ˜¯ä¸€äº›ä¾‹å­ï¼š
    ä¾‹å­1ï¼š'ä½ å¥½',æ–‡æœ¬åˆ†ç±»ç»“æœæ˜¯1
    ä¾‹å­2:'ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±',æ–‡æœ¬åˆ†ç±»ç»“æœæ˜¯1
    ä¾‹å­3ï¼š'ä½ æ˜¯è°',æ–‡æœ¬åˆ†ç±»ç»“æœæ˜¯1
    ä¾‹å­4ï¼š'ä»‹ç»ä¸€ä¸‹ä¸‰å›½æ¼”ä¹‰',æ–‡æœ¬åˆ†ç±»ç»“æœæ˜¯2
    ä¾‹å­4ï¼š'ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·',æ–‡æœ¬åˆ†ç±»ç»“æœæ˜¯2
    ä¾‹å­4ï¼š'ä»‹ç»ä¸€ä¸‹åä¾¨å¤§å­¦',æ–‡æœ¬åˆ†ç±»ç»“æœæ˜¯2
    è¯·å‚è€ƒä¸Šé¢ä¾‹å­ï¼Œç›´æ¥ç»™å‡ºä¸€ç§åˆ†ç±»ç»“æœï¼Œä¸è¦è§£é‡Šï¼Œä¸è¦å¤šä½™çš„å†…å®¹ï¼Œä¸è¦å¤šä½™çš„ç¬¦å·ï¼Œä¸è¦å¤šä½™çš„ç©ºæ ¼ï¼Œä¸è¦å¤šä½™çš„ç©ºè¡Œï¼Œä¸è¦å¤šä½™çš„æ¢è¡Œï¼Œä¸è¦å¤šä½™çš„æ ‡ç‚¹ç¬¦å·ï¼Œä¸è¦å¤šä½™çš„æ‹¬å·ã€‚
    è¯·ä½ å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œæ–‡æœ¬åˆ†ç±»ï¼š
    {question}
    """
    result = llm.predict(prompt)
    return result


bot = Chatbot()


# å®šä¹‰å·¥å…·
@tool
def get_date() -> str:
    "é€šè¿‡è¿™ä¸ªå·¥å…·å¯ä»¥è·å–ä»Šå¤©çš„æ—¥æœŸ"
    time = datetime.now().date().strftime('%Y-%m-%d')
    return time

@tool
def arxiv_search(question: str):
    "å½“ç”¨æˆ·éœ€è¦é€šè¿‡arxivæœç´¢å¼•æ“æŸ¥è¯¢æŒ‡å®šå…³é”®è¯çš„ç›¸å…³è®ºæ–‡æˆ–æ–‡ç« èµ„æ–™æ—¶ï¼Œå¯ä»¥è°ƒç”¨è¿™ä¸ªå·¥å…·"
    results = arxiv.Search(
        query=question[:300],
        max_results=1
    ).results()
    docs = [
        f'å‘è¡¨æ—¥æœŸ: {result.updated.date()}\næ ‡é¢˜: {result.title}\n'
        f'ä½œè€…: {", ".join(a.name for a in result.authors)}\n'
        f'æ–‡ç« æ‘˜è¦: {result.summary}'
        for result in results
    ]
    print("docs", docs)
    return docs


@tool
def make_img(question: str):
    "å½“ç”¨æˆ·éœ€è¦æ ¹æ®å…³é”®è¯ç”Ÿæˆå›¾ç‰‡çš„æ—¶å€™,å¯ä»¥ä½¿ç”¨è¿™ä¸ªå·¥å…·"
    # styles_option = [
    #     'dongman',  # åŠ¨æ¼«
    #     'guofeng',  # å›½é£
    #     'xieshi',   # å†™å®
    #     'youhua',   # æ²¹ç”»
    #     'manghe',   # ç›²ç›’
    # ]
    # aspect_ratio_options = [
    #     '16:9', '4:3', '3:2', '1:1',
    #     '2:3', '3:4', '9:16'
    # ]
    if ("åä¾¨å¤§å­¦" in question):
        output_path = bot.get_hqu_img(question)
        print("output_path", output_path)
        return output_path
    else:
        response = requests.post(
            url='https://magicmaker.openxlab.org.cn/gw/edit-anything/api/v1/bff/sd/generate',
            data=json.dumps({
                "official": True,
                "prompt": question,
                "style": "xieshi",
                "poseT": False,
                "aspectRatio": "4:3"
            }),
            headers={'content-type': 'application/json'}
        )
        image_url = response.json()['data']['imgUrl']
        image_response = requests.get(image_url)
        image = cv2.imdecode(np.frombuffer(image_response.content, np.uint8), cv2.IMREAD_COLOR)

        # tmp_dir = os.path.join("./img", 'tmp_dir')
        tmp_dir = "./web_demo/src/assets/llm_img"
        os.makedirs(tmp_dir, exist_ok=True)
        timestamp = int(time.time() * 1000)
        unique_id = uuid.uuid4().int
        unique_number = f"{timestamp}{unique_id}"
        output_path = os.path.join(tmp_dir, f"{unique_number}.jpg").replace("\\", "/")
        print("output_path", output_path)
        # ä¿å­˜è¾“å‡º
        cv2.imwrite(output_path, image)
        return output_path


@tool
def baidu_search(keyword: str) -> str:
    "é€šè¿‡æœç´¢å¼•æ“æŸ¥è¯¢æŒ‡å®šå…³é”®è¯çš„ç›¸å…³èµ„æ–™"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.115 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9",
        "Accept-Language": "zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7",
        "Connection": "keep-alive",
        "Accept-Encoding": "gzip, deflate",
        "Host": "www.baidu.com",
        # éœ€è¦æ›´æ¢Cookie
        "Cookie": "BIDUPSID=4EA09413AA348579EBC32BB4171DA6C4; PSTM=1685953325; BAIDUID=4EA09413AA34857921D22606F3465DE3:SL=0:NR=20:FG=1; BD_UPN=12314753; MCITY=-%3A; BDUSS=d4V1kzMVE5TC02V0dzSzBHR0pPZzZjY2xXZ05-YkxPSlFYOTlqVWtlekUxVjFtSVFBQUFBJCQAAAAAAAAAAAEAAAA3oAABd2FuZ3BlbmdlAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMRINmbESDZmRX; BDUSS_BFESS=d4V1kzMVE5TC02V0dzSzBHR0pPZzZjY2xXZ05-YkxPSlFYOTlqVWtlekUxVjFtSVFBQUFBJCQAAAAAAAAAAAEAAAA3oAABd2FuZ3BlbmdlAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMRINmbESDZmRX; H_PS_PSSID=60269_60278_60282_60287_60296_60253; H_WISE_SIDS=60269_60278_60282_60287_60296_60253; H_WISE_SIDS_BFESS=60269_60278_60282_60287_60296_60253; BAIDUID_BFESS=4EA09413AA34857921D22606F3465DE3:SL=0:NR=20:FG=1; BD_CK_SAM=1; PSINO=3; delPer=0; BA_HECTOR=810l01ak80aha1a101ah0401evoeh21j58flh1v; ZFY=65gS58gkPrLw3aFizT:BRHNeW:AN:A3yhdtUx81zQ4:Aky8:C; shifen[389215454364_77942]=1716797107; BCLID=8729107127908649389; BCLID_BFESS=8729107127908649389; BDSFRCVID=43DOJeC62iPknV7toafpMW8kd_IPxeRTH6ao8FEfGw7FPXyfJA7mEG0P2U8g0KuM8DJTogKKKgOTHICF_2uxOjjg8UtVJeC6EG0Ptf8g0f5; BDSFRCVID_BFESS=43DOJeC62iPknV7toafpMW8kd_IPxeRTH6ao8FEfGw7FPXyfJA7mEG0P2U8g0KuM8DJTogKKKgOTHICF_2uxOjjg8UtVJeC6EG0Ptf8g0f5; H_BDCLCKID_SF=JnIJoD8hJIK3fP36qRojh-40b2T22jnOM6T9aJ5nJDonDtTJKPbMQftXy4cUqPnbQjQkW-tbQpP-HJ7y3R6xytKN2h6waTcQBm6QKl0MLpQWbb0xyUQY3jDzbxnMBMni52OnapTn3fAKftnOM46JehL3346-35543bRTLnLy5KJtMDFRj5-hDTjyDaR-htRX54ofW-oofK-5sh7_bf--D4FrMUTptb3yKCcj2x0y2bbrOfJ9bq5xy5K_hN7l045xbbvlbUnX5qcNMn6HQT3mDlQbbN3i-xuO3gQnWb3cWhvJ8UbSyfbPBTD02-nBat-OQ6npaJ5nJq5nhMJmb67JD-50exbH55uHtRktVx5; H_BDCLCKID_SF_BFESS=JnIJoD8hJIK3fP36qRojh-40b2T22jnOM6T9aJ5nJDonDtTJKPbMQftXy4cUqPnbQjQkW-tbQpP-HJ7y3R6xytKN2h6waTcQBm6QKl0MLpQWbb0xyUQY3jDzbxnMBMni52OnapTn3fAKftnOM46JehL3346-35543bRTLnLy5KJtMDFRj5-hDTjyDaR-htRX54ofW-oofK-5sh7_bf--D4FrMUTptb3yKCcj2x0y2bbrOfJ9bq5xy5K_hN7l045xbbvlbUnX5qcNMn6HQT3mDlQbbN3i-xuO3gQnWb3cWhvJ8UbSyfbPBTD02-nBat-OQ6npaJ5nJq5nhMJmb67JD-50exbH55uHtRktVx5; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; B64_BOT=1; sug=3; sugstore=0; ORIGIN=0; bdime=0; H_PS_645EC=7da2HdXe6gMkTCYHhNWjWjBKf%2F91xi1Bc2ju7gIhGU8jSNH4YjL1%2BaTa6lI; baikeVisitId=c252a4ed-daf5-4079-8ca6-d287e0ffca21; COOKIE_SESSION=454_1_5_9_12_17_1_0_3_7_0_4_76645_0_0_0_1716797758_1716797108_1716799282%7C9%23409189_19_1716797105%7C9; BDSVRTM=34"
    }
    url = "https://www.baidu.com/s?wd=" + keyword
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text)
    text = ""
    for i in soup.find_all("div", class_="result c-container xpath-log new-pmd"):
        try:
            text = text + "tite:" + i.find("a").text + "\n"
            text = text + "content:" + i.find("span", class_="content-right_2s-H4").text + "\n"
        except:
            continue
    return text


@tool
def takeout():
    "å½“ç”¨æˆ·æƒ³è¦ç‚¹å¤–å–çš„æ—¶å€™ï¼Œå¯ä»¥è°ƒç”¨è¿™ä¸ªå·¥å…·"
    foods = [
        {"name": "å®«ä¿é¸¡ä¸é¥­", "price": 15.00},
        {"name": "é±¼é¦™è‚‰ä¸é¥­", "price": 14.00},
        {"name": "å‰çƒ§é¥­", "price": 20.00},
        {"name": "ç‰›è‚‰æ‹‰é¢", "price": 16.00},
        {"name": "æ°´é¥º", "price": 12.00}
    ]
    selected_food = random.choice(foods)
    result = f"å·²ç»ä¸ºæ‚¨ç‚¹äº†{selected_food['name']},ä»·æ ¼ä¸º{selected_food['price']}å…ƒ,å·²è‡ªåŠ¨ä»æ‚¨å­¦ç”Ÿå¡æ”¯ä»˜"
    return result


@tool
def library() -> str:
    "å½“ç”¨æˆ·æƒ³è¦é¢„å®šå›¾ä¹¦é¦†åº§ä½çš„æ—¶å€™å¯ä»¥ä½¿ç”¨è¿™ä¸ªå·¥å…·"
    # éšæœºé€‰æ‹©æ¥¼å±‚
    floor = random.randint(1, 5)
    # éšæœºé€‰æ‹©åº§ä½å·
    seat_number = random.randint(1, 99)
    seat = f"{floor}-{seat_number}"
    result = f"å·²ç»ä¸ºæ‚¨æ£€ç´¢åˆ°äº†ç©ºé—²åº§ä½ï¼Œé¢„å®šäº†{seat}å·åº§ä½ï¼Œè¯·åŠæ—¶å‰å¾€å°±å"
    return result


tool_list = [get_date, baidu_search, arxiv_search, make_img, library, takeout]


def tool_chain(model_output):
    tool_map = {tool.name: tool for tool in tool_list}
    chosen_tool = tool_map[model_output["name"]]
    return itemgetter("arguments") | chosen_tool


new_tool_list = [convert_to_openai_tool(i) for i in tool_list]

TOOL_DESC = """
{tool_name}: å·¥å…·æè¿°:{tool_description} parameters: {tool_parameters} Format the arguments as a JSON object.
"""

REACT_PROMPT = """
You are an assistant who is good at calling external tools. 
Try your best to answer the following questions in Chinese. 
The tools you can use include:

{tool_descs}

Use the following format:

Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, must be one of [{tool_names}]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can be repeated zero or more times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question

Begin!

Question: {query}
Thought:
"""


def build_planning_prompt(query):
    tool_descs = []
    tool_names = []
    for tools in new_tool_list:
        tool_descs.append(
            TOOL_DESC.format(
                tool_name=tools["function"]["name"],
                tool_description=tools["function"]["description"],
                tool_parameters=json.dumps(
                    tools["function"]["parameters"], ensure_ascii=False
                )
            )
        )
        tool_names.append(tools["function"]["name"])
    tool_descs = '\n\n'.join(tool_descs)
    tool_names = ','.join(tool_names)
    prompt = REACT_PROMPT.format(tool_descs=tool_descs, tool_names=tool_names, query=query)
    return prompt


def get_res(prompt):
    response = llm.predict(prompt)
    return response


parser = JsonOutputParser()


def get_args(res):
    args0 = res.split("\nAction: ")[1].split("\nAction Input:")
    return {"name": args0[0], "arguments": parser.parse(args0[1])}


class Content(BaseModel):
    content: str


@app.post("/send_question")
async def send_question(content: Content):
    global source
    source = []
    question = content.content
    fl_res = query_classify(question)
    print("fl_res", fl_res)
    if fl_res == '1':
        _, result = get_same_response(self_qa())

        def generate():
            for text in result:
                yield str(text.content)

        return StreamingResponse(generate(), media_type="text/event-stream")
    else:
        is_stream, result = bot.get_response(question)
        print("is_stream", is_stream)
        if is_stream == 0:
            print('result', result.replace('"', ''))
            return result
        else:
            def generate():
                for text in result:
                    yield str(text.content)

            return StreamingResponse(generate(), media_type="text/event-stream")


@app.post("/send_exam_question")
async def send_exam_question(content: Content):
    question = content.content
    answer = bot.get_pr_exam_response(question)

    def add_spaces_to_brackets(text):
        # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…è¿ç»­çš„èŠ±æ‹¬å·
        pattern = r'(\{|\})+'
        # replaced_text = re.sub(r'\\(right|left)', r'\\lvert', text)
        # ä½¿ç”¨re.subå‡½æ•°æ›¿æ¢åŒ¹é…çš„è¿ç»­èŠ±æ‹¬å·ï¼Œåœ¨å®ƒä»¬ä¹‹é—´æ·»åŠ ç©ºæ ¼
        result = re.sub(pattern, lambda m: ' '.join(m.group()), text)
        return result

    # return answer
    def generate():
        for text in answer:
            text.content = add_spaces_to_brackets(text.content)
            print(text.content)
            yield str(text.content)

    return StreamingResponse(generate(), media_type="text/event-stream")


@app.post("/whisper")
async def to_whisper(file: UploadFile = File(...)):
    filename = file.filename
    file_content = await file.read()
    with open(f"./{filename}", "wb") as f:
        f.write(file_content)
    model = whisper.load_model("small")
    audio = whisper.load_audio(f"./{filename}")
    audio = whisper.pad_or_trim(audio)
    mel = whisper.log_mel_spectrogram(audio).to(model.device)
    _, probs = model.detect_language(mel)
    # print(f"Detected language: {max(probs, key=probs.get)}")
    options = whisper.DecodingOptions(prompt="ä»¥ä¸‹æ˜¯æ™®é€šè¯çš„å¥å­")
    result = whisper.decode(model, mel, options)
    print(result.text)
    return result.text


# å¾—åˆ°é—®é¢˜æ¥æºçš„æ¥å£
@app.post("/get_source")
async def get_source():
    global source
    source = source[-3:]
    return source


class Summarize(BaseModel):
    question: str
    answer: str


# æ€»ç»“çš„æ¥å£
@app.post("/summarize")
async def llmSummarize(summarize: Summarize):
    question = summarize.question
    answer = summarize.answer
    # print(question,answer)
    prompt = f"""
    ä½ çš„ä»»åŠ¡æ˜¯ï¼Œæ ¹æ®é—®é¢˜å’Œç­”æ¡ˆï¼Œç”Ÿæˆä¸€ä¸ªèƒ½å¤Ÿæ¦‚æ‹¬è¯¥å¯¹è¯çš„æ ‡é¢˜ï¼Œ
    è¿™å¯¹æˆ‘æ¥è¯´ååˆ†é‡è¦ã€‚
    å¦‚æœä½ åšçš„å¥½ï¼Œæˆ‘ä¼šç»™ä½ ä¸€å®šçš„èµé‡‘ã€‚
    é—®é¢˜:{question},
    ç­”æ¡ˆ:{answer},
    ä½ æ¦‚æ‹¬çš„æ ‡é¢˜:
    """
    result = llm.predict(prompt)
    # print(result)
    return result


class ToTTS(BaseModel):
    content: str


@app.post("/tts")
async def tts(data: ToTTS):
    TEXT = data.content
    VOICE = "zh-CN-XiaoxiaoNeural"
    OUTPUT_FILE = "./test.mp3"
    if os.path.exists(OUTPUT_FILE):
        os.remove(OUTPUT_FILE)
    communicate = edge_tts.Communicate(TEXT, VOICE)
    await communicate.save(OUTPUT_FILE)
    return FileResponse(OUTPUT_FILE, media_type="audio/mpeg")


@app.post("/send_emo_question")
async def send_emo_question(content: Content):
    question = content.content
    answer = bot.get_Emo_response(question)

    def generate():
        for text in answer:
            yield str(text.content)

    return StreamingResponse(generate(), media_type="text/event-stream")

# if __name__ == '__main__':
#     import uvicorn
#     uvicorn.run("main:app", host="127.0.0.1", port=7070, reload=True)
# uvicorn main:app --host 127.0.0.1 --port 7091 --workers 1 --reload
